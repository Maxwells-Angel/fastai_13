{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556f7df9-535b-407f-b1f2-a5940dc83323",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8efa70a-e1a4-445b-9284-1dc7f21bf850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, time, torch, os, matplotlib as mpl, shutil, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor \n",
    "from fastcore.test import test_close\n",
    "\n",
    "# important! so I can compare my work\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "\n",
    "with gzip.open(path_gz, 'rb') as f: \n",
    "   ((x_train,y_train),(x_valid,y_valid),_) = pickle.load(f, encoding = 'latin-1')\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = map(tensor,(x_train,y_train,x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f49fb3-bb4b-4652-a907-042f79cffe1b",
   "metadata": {},
   "source": [
    "# Basic Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d78d2-0580-4468-8a45-64fb2d55e5de",
   "metadata": {},
   "source": [
    "We will use the basic shape of the input to define the architecture of our layers. \n",
    "\n",
    "Following the lesson, we'll create 2 linear layers with a activation layer (reLu) in between. \n",
    "\n",
    "The first layer with have 784 neurons to match the 784 inputs. It will have 50 hidden neurons. The second layer will have 50 inputs and 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4abfe4c-4df3-4394-809f-54dc7fb5f698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000]), torch.Size([50000, 784]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6298c1-ea2d-4280-b932-6dd846df73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252e17d0-18ce-45fb-88ed-5ae23e25d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(784,50)\n",
    "b1 = torch.randn(50)\n",
    "w2 = torch.randn(50,1)\n",
    "b2 = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f288eb8d-b0cf-465d-834c-7d4b8171212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.randn(nh)\n",
    "w2 = torch.randn(nh,1)\n",
    "b2 = torch.randn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e7e0b-2141-4e79-963f-a0077fd4d2af",
   "metadata": {},
   "source": [
    "The above was my first implementation. Not wrong, but I notice that jeremy initialized the biases with `torch.zeros(*size)` rather than `torch.randn(*size)`. Does that make any difference? I'll keep mine for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafce17-cfbe-4ed8-a9a5-6de1a266b074",
   "metadata": {},
   "source": [
    "Jeremy implemented the model in three ways:\n",
    "\n",
    "1. Functions\n",
    "2. Classes\n",
    "3. Classes with inheritance\n",
    "\n",
    "Let's begin by creating functions for the linear operation and for reLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43d60e9-5c5b-4f27-8ea8-00cb5c6acb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(xb,w,b): return xb@w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8467c61b-27c3-4580-8432-c7316870d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(inp): return inp.clamp_min(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18d76f-6ba9-4ef6-ae25-dae9dabcfa84",
   "metadata": {},
   "source": [
    "Seems okay, why don't we try it out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5915ec9f-4233-4f10-b69f-4f0c9150de09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.03, -11.21,   1.34,  ...,  11.02,   3.02,  -3.79],\n",
       "        [ -3.41, -12.25,  -7.79,  ...,  31.63,   9.03, -11.11],\n",
       "        [  5.08,  -0.43,  -6.60,  ...,   5.51,  -5.56,  -7.53],\n",
       "        ...,\n",
       "        [ -2.86, -12.46,   0.61,  ...,  14.29,  -5.90,  -6.44],\n",
       "        [  7.43, -19.30, -10.29,  ...,   4.37,  -0.15,  -7.13],\n",
       "        [  9.10, -18.73,   6.79,  ...,   0.36, -16.52, -14.49]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter = linear(x_train,w1,b1)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021ab75e-2a53-4a47-9c68-111b8693bf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.03,  0.00,  1.34,  ..., 11.02,  3.02,  0.00],\n",
       "        [ 0.00,  0.00,  0.00,  ..., 31.63,  9.03,  0.00],\n",
       "        [ 5.08,  0.00,  0.00,  ...,  5.51,  0.00,  0.00],\n",
       "        ...,\n",
       "        [ 0.00,  0.00,  0.61,  ..., 14.29,  0.00,  0.00],\n",
       "        [ 7.43,  0.00,  0.00,  ...,  4.37,  0.00,  0.00],\n",
       "        [ 9.10,  0.00,  6.79,  ...,  0.36,  0.00,  0.00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = relu(inter)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3597d7b-5561-425c-8186-94ea655738dd",
   "metadata": {},
   "source": [
    "Looks like it works! But I might want to do another linear layer. Where do I fit that in? I just call linear again with different args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328ee55c-eed1-41b7-881f-0903318c68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = linear(preds,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d5d3fc-7d00-4653-bd79-9244c9b40db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456dcfe-6764-4704-b19b-cc058efe2e9c",
   "metadata": {},
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d25e0-9e10-4b70-9099-c1c4357bbf44",
   "metadata": {},
   "source": [
    "Cool! That works. We get a tensor of the right shape, but we don't just want predictions, we need a way to compute the loss. Let's write a function to compute the loss. \n",
    "\n",
    "Again, following Jeremy, I will use mse for the loss function even though that is totally inappopriate for this kind of prediction. After we've got backpropagation working properly, we'll move onto a better loss function like cross entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72a2799-6410-4265-8c5a-b93300029e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targs):\n",
    "    return (output[:,0]-targs).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999462bd-8d2e-4181-8898-3410b9f8f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1590.84)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds,y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f461c08-fc4f-45ea-b806-487c3c4dc65b",
   "metadata": {},
   "source": [
    "This answer is different from Jeremy's. I've made a few deviations that might explain why. \n",
    "1. I didn't convert my y_train to float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eae5ed84-2cbe-4186-af7e-7cab26f90b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 0., 4.,  ..., 8., 4., 8.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27049802-742d-46cc-81eb-dcef3beddb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1590.84)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds,y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fec4f0-6461-4fee-b52c-198108dcb309",
   "metadata": {},
   "source": [
    "Same thing! I'm going to try restarting the kernel. Didn't work. I got the same answer. Hmmm. Well, I'm not sure I have any other reason to suspect this is wrong. So I'll roll with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a0019-4f2a-48a8-b5f6-7d6467ebb8f1",
   "metadata": {},
   "source": [
    "The other thing I did differently was that I didn't roll my layers into a model. I'll follow him on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4d3946-35fd-4c6c-855d-b7773a09436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb): \n",
    "    l1 = linear(xb,w1,b1)\n",
    "    l2 = relu(l1)\n",
    "    return linear(l2,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "748da91a-396e-4acd-9d56-c24d8ae7450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a2b4488-431b-41b5-9adc-fb416c340317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   -43.42],\n",
       "        [   -99.04],\n",
       "        [    16.26],\n",
       "        ...,\n",
       "        [   -41.02],\n",
       "        [   -11.81],\n",
       "        [    -0.01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7fe6e00-b91a-400d-ba23-1301f8618b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   -43.42],\n",
       "        [   -99.04],\n",
       "        [    16.26],\n",
       "        ...,\n",
       "        [   -41.02],\n",
       "        [   -11.81],\n",
       "        [    -0.01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4a4cf-42f7-429a-b4f9-4917a06b8202",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Quick Digression - Loss Verification\n",
    "How can I be sure my loss is correct? Compute it manually on a small set and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d34e480-6916-4803-8994-acfa463d4668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([5]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst = x_train[:5]\n",
    "y_tst = y_train[:5]\n",
    "x_tst.shape,y_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03858e30-1226-4992-9a2b-e0fc553068cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d168a5-289f-4982-b0b1-53e321e7f387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-43.42],\n",
       "        [-99.04],\n",
       "        [ 16.26],\n",
       "        [ 15.32],\n",
       "        [-40.14]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds3 = model(x_tst)\n",
    "preds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ac4d6d4-22e8-4812-a2c9-54e24f137397",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_loss = mse(preds3,y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "301683a7-da04-4373-8c2a-7ea4bb2adb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2984.56)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b4e83d5-0a8b-4385-81bb-d8d54408bf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   -43.42,    -99.04,     16.26,  ...,    -41.02,    -11.81,     -0.01])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0367cd4-4180-42ee-8161-6239f79ec276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-48.42, -99.04,  12.26,  14.32, -49.14])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = preds3[:,0]-y_tst\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8addd73-7186-4d44-8eb2-817ad1d175d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2344.19, 9808.50,  150.37,  205.19, 2414.57])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4715f348-3c17-41ea-b2ec-993f66b76411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2984.56)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f157f88-eea5-4084-a332-def9465df820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2344.4964"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-48.42*-48.42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ae1f7-c9dd-46e5-87c8-3a3e0c58d980",
   "metadata": {},
   "source": [
    "Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff08e43-f899-491f-bcdf-68d70a51a547",
   "metadata": {},
   "source": [
    "# Gradients and Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4db429cd-7cf3-495b-9c05-3319b0557c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b): \n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = inp.t()@out.g\n",
    "    #w.g = inp.T@out.g \n",
    "    #-- this method doesn't work for all cases. It runs into trouble at the highest layers where the output and input tensors become rank 1\n",
    "    # in that case, there is nothing to transpose and the axes will be different sizes. I'm confused. Why doesn't the above have the same problem?\n",
    "    # This method, by contrast, will creates unit axes that allow us to broadcast operations \n",
    "    #import pdb; pdb.set_trace()\n",
    "    w.g = (inp.unsqueeze(-1)*out.g.unsqueeze(1)).sum(0)\n",
    "    # actually, the above methods work-- I just need to remember to do a matrix multiply '@' instead of elementwise opp '*'\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3f2e8-475e-4ae5-a9c3-d011858f688a",
   "metadata": {},
   "source": [
    "Why do we sum for gradients of w and b? We want to integrate over the relative contributions for each input. \n",
    "\n",
    "To put it another way, each weight/biases has some impact on the loss relative to its input. I.e., the derivative of the loss with respect to w1 is x for this input and is y for input 2 and z for input 3, etc. We want w1's total derivative, one equation, so we need to sum that contribution over all the inputs to get the true contribution of w1 (and all the other weights). \n",
    "\n",
    "Each input, on the other hand, is not seperated across weights. The weights are aligned to the input, that's why the matrix matplication works. so there's no need to sum. Each input already gets its derivative handled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae1c7c0d-ad19-476b-97b6-23e7980ca757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = linear(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = linear(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e631bfa-59b3-4db2-aca2-0c92989742b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9098645a-5172-47e1-8837-b892826b9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(x): return x.g.clone()\n",
    "\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g, w2g,b1g,b2g,ig = tuple(map(get_grad,chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30ff9846-6999-4c6c-b337-5d112a75c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad,chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ea3cb438-1244-4e69-86ae-6dfad4b96448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ): \n",
    "    l1 = linear(inp, w12, b12)\n",
    "    l2 = relu(l1) \n",
    "    out = linear(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d1759f4e-8de3-4cf8-9624-bcce1190aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "989aed01-d99b-4a00-be5f-98852a0e230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): test_close(a,b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b311112-4353-4a60-8150-e9ce7d22ffdc",
   "metadata": {},
   "source": [
    "Ok! Where do do I go from here? If I remember correctly -- Jeremy wrote out each of his layers as classes so that they could hold information about their gradients. There was some discussion of chain rule as well. I remember this being confusing for me, so I will solider on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fc88c7f-62f0-4151-b65c-866281844d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(): \n",
    "    def __call__(self, inp, targs):\n",
    "        self.inp,self.targs = inp,targs\n",
    "        self.out = (inp[:,0]-targs).pow(2).mean()\n",
    "        return self.out\n",
    "        \n",
    "    def backward(self):\n",
    "        self.inp.g = 2*((output[:,0]-targs))/output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23a7304a-8371-48cd-ab1e-65cdfdd55160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(): \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0)\n",
    "        return self.out\n",
    "        \n",
    "    def backward(self):\n",
    "        self.inp.g =(self.inp>0).float()*self.out.g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "053b3875-4561-4362-8f7f-4e9dba3b5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self,w,b):\n",
    "        self.w = w\n",
    "        self.b = b \n",
    "        \n",
    "    def __call__(self, inp): \n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w+self.b \n",
    "        return self.out\n",
    "        \n",
    "    def backward(self):\n",
    "        self.w.g = inp.t()@self.out.g\n",
    "        self.inp.g = self.out.g@self.w.T\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bc725fc-fee9-47d1-9162-dc72032716eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,w1, w2,b1,b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "\n",
    "    def __call__(self, x, targ):\n",
    "        \n",
    "        # forward pass\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        \n",
    "        # init backward pass with loss calculation\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        # continue backward pass\n",
    "        self.loss.backward()\n",
    "        \n",
    "        # call backward()-->.bwd() on each layer\n",
    "        for l in reversed(self.layers): \n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "190f32f3-ea25-41d1-adfd-52659b1747d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1,w2,b1,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "585a3e05-0891-4c0c-af62-133a0ee32d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78a1c8d0-0852-4647-96cf-4ed6927696cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1590.84)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a53967-eefb-480b-b080-ae24cf364ad0",
   "metadata": {},
   "source": [
    "## Module.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43512329-bd77-4e14-919c-87c6f6538e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Class for Inheritance\n",
    "class Module():    \n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf8b707d-c444-4c0c-9457-79c5520dbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0)\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a063ae2b-ddaa-4248-bccf-9fab4e5d0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module): \n",
    "    def __init__(self, w,b): self.w, self.b = w,b \n",
    "    def forward(self, inp): return inp @ self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ee0df99-fed4-4dfe-ad71-951dfa9f255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module): \n",
    "    def forward(self, inp, targ): return (inp[:,0]-targ).pow(2).mean(0)\n",
    "    def bwd(self,out,inp,targ): inp.g = 2*(inp[:,0]-targ).unsqueeze(-1)/inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fc04846-dbe8-4ce8-8bd8-b2340823448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1,w2,b1,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30f18784-0a63-409b-9cdf-ba78d6ff9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19527d91-bbd7-4f0b-98c4-8f359b3389c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9869c-691b-49e2-9e58-62d24730b7f2",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b39a6ba5-c399-454f-b474-aaec3df0f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7ff77d4-cdca-450e-a0f5-f93322928942",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.float()\n",
    "x_train = x_train.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea78e478-db42-4995-8f73-839511a85947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module): \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self,inp): return inp@self.w+self.b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a221cfe-cf8f-4ad7-9caf-0adb941224f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targs):\n",
    "        \n",
    "        # forward pass\n",
    "        for l in self.layers: \n",
    "            x = l(x) \n",
    "         \n",
    "        return F.mse_loss(x, targs[:,None]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b2b5787b-b67b-4ea4-9031-27f6cfa7a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x_train.shape[1],50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b48c6f00-d3c4-4493-b1b5-54ebb4453c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70491c89-90d0-4d4d-b6bc-7ef6c3736fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c763b7e-e15d-4dd9-8297-28d1cb746886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.35, -61.78,  14.58,   5.10,  69.91, -78.21,  50.94, 276.71, 214.24,  16.58, 228.04,  48.73, -26.02, 197.18,\n",
       "        129.62, -26.05, -49.19, 127.78, -28.31,   2.96, -22.60, -23.90, -63.58,  -0.47,  35.05, -41.11,  14.80,  26.05,\n",
       "        -11.11, -68.51, -13.66, 264.30,  24.00,  -4.00,  17.84, 117.48, -15.01, -50.04, 116.87,  40.59,  28.73, -91.12,\n",
       "         21.37,  32.92, -40.96, -24.52,  37.49, -97.57, 137.62, 127.02])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
